# SAFFRONITES-INNO-2225
Detailed Description:

Lack of hearing and speech can affect the exchange of information. People with hearing and speech disabilities use different modes of communication to communicate with people. One of the globally accepted ways to convey information with the deaf and dumb community and with other people is sign language. Sign language is the visual language used to convey meaning through hand gestures. Various algorithms and techniques in Machine Learning have been developed to translate sign language gestures in to its corresponding text output. We propose a prototype where we have applied deep learning neural networks for predicting the Indian sign language's gestures. And further, we have used the OpenCV library for capturing the image, MediaPipe module for detecting the hand landmarks, and Recurrent Neural Network (RNN) for training the model.

The proposed methodology includes:
1.Data gathering
2.Data pre-processing
3.Segmentation
4.Extraction of landmarks
5.Train and Test models
6.Classification of different gestures
7.Model evaluation using a confusion matrix

Current Status:

We have created our own Dataset of Indian Sign Language's gestures from 0 to 9.

